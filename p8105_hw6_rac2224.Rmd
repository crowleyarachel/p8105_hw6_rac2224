---
title: "p8105_hw6_rac2224"
author: "Rachel Crowley"
date: "11/24/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

options(ggplot2.continuous.colour = "viridis",
        ggplot2.continuous.fill = "viridis"
        )

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))

set.seed(34565)
```

## Problem 1

Loading and cleaning the data for regression analysis 

```{r}
bwt_data = read_csv("data/birthweight.csv") %>%
  rename(
    baby_sex = babysex,
    baby_head = bhead,
    baby_length = blength,
    birthweight = bwt,
    delivery_weight = delwt,
    family_income = fincome,
    father_race = frace,
    gestational_age = gaweeks,
    malformations = malform,
    menarche_age = menarche,
    mother_height = mheight,
    mother_age = momage, 
    mother_race = mrace,
    previous_low_birthweight = pnumlbw,
    prior_small_for_gestational = pnumsga, 
    pre_pregnancy_bmi = ppbmi,
    pre_pregnancy_weight = ppwt,
    smoking_during_pregnancy = smoken, 
    pregnancy_weight_gain = wtgain
  ) %>%
  mutate(
    baby_sex = as.factor(baby_sex),
    father_race = as.factor(father_race),
    malformations = as.factor(malformations),
    mother_race = as.factor(mother_race) )%>%
  mutate(
    baby_sex = recode(baby_sex, '1' = 'Male', '2' = 'Female'),
    father_race = recode(father_race, '1' = 'White', '2' = 'Black', '3' = 'Asian', '4' = 'Puerto Rican', '8' = 'Other', '9' = 'Unknown'),
    malformations = recode(malformations, '0' = 'Absent', '1' = 'Present'),
    mother_race = recode(mother_race, '1' = 'White', '2' = 'Black', '3' = 'Asian', '4' = 'Puerto Rican', '8' = 'Other')
  ) %>%
  mutate(
    baby_sex = fct_infreq(baby_sex),
    father_race = fct_infreq(father_race),
    malformations = fct_infreq(malformations),
    mother_race = fct_infreq(mother_race)
  )
 
colSums(is.na(bwt_data))

```

* baby_sex, mother_race, father_race, and malformations were converted from numeric to factor variables. There were no missing values across all variables in the dataset. 

Building regression models 

* For the proposed regression model for birthweight, the following variables were included in the model as predictors, based on existing literature that support an associaton between these factors and birthweight:  average number of cigarettes smoked per day during pregnancy (smoking_during_pregnancy), mother's age at delivery in years (mother_age), mother’s pre-pregnancy BMI (pre_pregnancy_bmi), and gestational age in weeks (gestational_age). It is hypothesized that for a one unit increase in the average number of cigarettes smoked per day during pregnancy, birthweight will decrease; for an increase in age at delivery in years, birthweight will decrease; for a one unit increase in mother's pre-pregnancy BMI, birthweight will increase; for an increase in gestational age in weeks, birthweight will increase. 

Fitting the model and plot of model residuals against fitted value

```{r}

fit = lm(birthweight ~ smoking_during_pregnancy + mother_age + pre_pregnancy_bmi + gestational_age, data = bwt_data)

summary(fit)$coef

fit %>% 
  broom::tidy() %>%
   knitr::kable(digits = 3)

bwt_data %>%
  modelr::add_residuals(fit) %>%
  modelr::add_predictions(fit) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + 
  geom_hline(aes(yintercept = 0)) + 
  labs(
    title = "Model residuals against fitted values for proposed regression model for birthweight",
    y = "Residuals",
    x = "Fitted Values",
    caption = "Data from Birthweight.csv"
  )
```

*When modeling residuals against fitted values for the proposed regression model, the residuals are scattered far away from the line where residuals (y) equals 0, indicating high variance and difference between observed and predicted values with this model. 

Fitting two other models 
 
```{r}
fit2 = lm(birthweight ~ baby_length + gestational_age, data = bwt_data)

summary(fit2)$coef

fit2 %>% 
  broom::tidy() %>%
   knitr::kable(digits = 3)

bwt_data %>%
  modelr::add_residuals(fit2) %>%
  modelr::add_predictions(fit2) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + 
  geom_hline(aes(yintercept = 0)) + 
  labs(
    title = "Model residuals against fitted values for regression model with baby length and gestational age",
    y = "Residuals",
    x = "Fitted Values",
    caption = "Data from Birthweight.csv"
  )

fit3 = lm(birthweight ~ baby_head*baby_length*baby_sex, data = bwt_data)


summary(fit3)$coef

fit3 %>% 
  broom::tidy() %>%
   knitr::kable(digits = 3)

bwt_data %>%
  modelr::add_residuals(fit3) %>%
  modelr::add_predictions(fit3) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + 
  geom_hline(aes(yintercept = 0)) + 
  labs(
    title = "Modeling residuals against fitted values for regression model with head size, length, and sex interaction",
    y = "Residuals",
    x = "Fitted Values",
    caption = "Data from Birthweight.csv"
  )

```

* When modeling residuals against fitted values for regression model with head size, length, and sex interaction and for the regression model with interaction by baby's head size, length, and sex, the residuals are closer to the line where residuals equals zero (y=0), indicating less variance with these models compared to the proposed model above. These models still have reiduals that are far away from 0, thus variance is still moderately high with these models. The model with baby's head size, length, and sex appears to have the least variance of the three models, as residual values are closer to the line and fewer extreme values exist. 

Comparing regression models in terms of the cross-validated prediction error

```{r, warning = F}

cv_df = 
  crossv_mc(bwt_data, 100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble) 
  ) %>%
  mutate(
    linear_mod1  = map(train, ~lm(birthweight ~ smoking_during_pregnancy + mother_age + pre_pregnancy_bmi + gestational_age,     data = bwt_data)),
    linear_mod2  = map(train, ~lm(birthweight ~ baby_length + gestational_age, data = bwt_data)),
    linear_mod3  = map(train, ~lm(birthweight ~ baby_head*baby_length*baby_sex, data = bwt_data))) %>% 
  mutate(
    rmse_linear1 = map2_dbl(linear_mod1, test, ~rmse(model = .x, data = .y)),
    rmse_linear2 = map2_dbl(linear_mod2, test, ~rmse(model = .x, data = .y)),
    rmse_linear3 = map2_dbl(linear_mod3, test, ~rmse(model = .x, data = .y))) %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse", 
    names_prefix = "rmse_") %>%
      ggplot(aes(x = model, y = rmse)) + geom_violin() + labs(
        title = "Root mean squared error distribution by model type",
        x = "Model",
        y = "Root Mean Squared Error: RMSE",
        caption = "Data from birthweight.csv"
      )
  
cv_df
```

* After plotting the prediction error distribution for each of the three models, the model with interaction by baby's head size, length, and sex has a higher density at the smallest values of root mean squared error and best fit compared to the other models. The proposed model has the highest root mean squared error values and worst fit, while the model with baby length and gestational age main effects has slightly worse fit than the model with interaction terms. 

## Problem 2

Understanding the distribution of r̂2 and log(β̂0∗β̂1) with simple linear regression and bootstrapping

Simple linear regression with tmax as the response and tmin as the predictor, finding beta_0, beta_1, log(beta_0*beta_1), and r^2 estimates 


```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

boot_straps = weather_df %>%
  bootstrap(n = 5000) %>%
  mutate(
  linear_models = map(strap, ~lm(tmax~tmin, data = .x)),
  tidy_results = map(linear_models, broom::tidy),
  glance_results = map(linear_models, broom::glance)
  ) %>%
  unnest(tidy_results, glance_results) %>%
  select(estimate, term, r.squared) %>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>%
  rename(
    beta_0 = `(Intercept)`,
    beta_1 = tmin,
    r_squared = r.squared
  ) %>%
  select(r_squared, beta_0, beta_1) %>%
  mutate(
    log_beta0_x_beta1 = log(beta_0*beta_1)
  )

```

Plotting the distribution of log(beta_0*beta_1) and r^2 estimates, calculating confidence intervals 

```{r}

boot_straps %>%
    ggplot(aes(x = log_beta0_x_beta1)) + geom_density() + geom_vline(xintercept = 2.016) + labs(
      title = "Distribution of log(beta0*beta1) estimates from a linear regression of tmax~tmin and 5000 bootstrap samples",
      x = "log(beta0*beta1)",
      caption = "Data from NOAA weather dataset"
    )

boot_straps %>%
    ggplot(aes(x = r_squared)) + geom_density() + geom_vline(xintercept = .912) + labs(
      title = "Distribution of r-squared estimates from a linear regression of tmax~tmin and 5000 bootstrap samples",
      x = "R-squared",
      caption = "Data from NOAA weather dataset"
    )

```

Distribution of log(beta_0*beta_1)

* The distribution of log(beta_0*beta_1) appears to have the form of a normal distribution, with the majority of points centered around a value of 2.016. 

* The 95% confidence interval for log(beta0xbeta1) is `r round(quantile(pull(boot_straps, log_beta0_x_beta1), probs = c(.025, .975)), digits = 3)`, thus we are 95% confident that the true log(beta0xbeta1) lies between 1.965 and 2.058. 

Distribution of r^2 estimates

* The majority of r^2 estimates are centered around a r^2 value of .912 and the distribution appears approximately normal. The high density of values around .912 provides evidence that a large percent of the variation in tmax can be explained by tmin. 

* The 95% confidence interval for the r_squared estimate is `r round(quantile(pull(boot_straps, r_squared), probs = c(.025, .975)), digits = 3)`, thus we are 95% confident that the true r_squared value lies between 0.893 and 0.927.
 
